{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "\n",
    "3. [Required libraries](#Required-libraries)\n",
    "\n",
    "4. [The problem domain](#The-problem-domain)\n",
    "\n",
    "5. [Step 1: Answering the question](#Step-1:-Answering-the-question)\n",
    "\n",
    "6. [Step 2: Checking the data](#Step-2:-Checking-the-data)\n",
    "\n",
    "7. [Step 3: Tidying the data](#Step-3:-Tidying-the-data)\n",
    "\n",
    "8. [Step 4: Rock Type Analysis](#Step-4:-Rock-Type-Analysis)\n",
    "\n",
    "11. [Conclusions](#Conclusions)\n",
    "\n",
    "11. [Acknowledgements](#Acknowledgements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "In this notebook, we will go over a basic Python data analysis pipeline from start to finish to show you what a typical data science workflow looks like for computing the Rock Type from a synthetic well log data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required libraries\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "This notebook uses several Python packages that come standard with the Anaconda Python distribution. The primary libraries that we'll be using are:\n",
    "\n",
    "* **NumPy**: Provides a fast numerical array structure and helper functions.\n",
    "* **pandas**: Provides a DataFrame structure to store data in memory and work with it easily and efficiently.\n",
    "* **scikit-learn**: The essential Machine Learning package in Python.\n",
    "* **matplotlib**: Basic plotting library in Python; most other Python plotting libraries are built on top of it.\n",
    "* **Seaborn**: Advanced statistical plotting library.\n",
    "* **watermark**: A Jupyter Notebook extension for printing timestamps, version numbers, and hardware information.\n",
    "* **pandas-profiling**: An open source Python module with which we can quickly do an exploratory data analysis with just a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem domain\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "For the purposes of this exercise, we will analyze the well log data with the essential measurements acquired by a logging tool. The aim is to carry on Rock type analysis using these input measurements. The data however is not clean and needs to be looked at before jumping into any analysis. <br>\n",
    "\n",
    "**Note:** The data set we're working with is synthetic and has been modified for demonstration purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Answering the question\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "The first step to any data analysis project is to define the question or problem we're looking to solve, and to define a measure (or set of measures) for our success at solving that task. The data analysis checklist makes us answer a handful of questions to accomplish that.\n",
    "\n",
    ">Did you specify the type of data analytic question (e.g. exploration, association causality) before touching the data?\n",
    "\n",
    "We're trying to classify the rock type based on the log measurements: <br >``'GammaRay(API)', 'ShaleVolume', 'Resistivity', 'Sonic Delta-T', 'Vp(m/s)', 'Vs', 'Density', 'NeutronPorosity', 'DensityPorosity(g/cc)', 'PoissonRatio'.``\n",
    "\n",
    ">Did you understand the context for the question and the scientific or business application?\n",
    "\n",
    "We're building part of a data analysis pipeline to automate the process of rock type identification. In the future, this pipeline will be connected to another pipeline that automatically highlights what kind of rock we are drilling through. This can help take important decisions on the drilling parameters to use, mud pressure that needs to be maintained etc.\n",
    "\n",
    ">Did you record the experimental design?\n",
    "\n",
    "The well log data is acquired by the logging tool during or after drilling the well. Different tools provide different measurements which behave differently depending on the nature of the rock being drilled. Analysis of the well logs together gives us the information about what kind of a rock we would be drilling. Machine Learning approach can automate the process of identification of rock.\n",
    "\n",
    ">Did you consider whether the question could be answered with the available data?\n",
    "\n",
    "The dataset we have is a 330m well section and would contain rock types specific to the interval under consideration. Once these rock types are identified based on the input measurements, same model can be applied on another section to identify any overlap or similar rock types. \n",
    "\n",
    "<hr />\n",
    "\n",
    "Notice that we've spent a fair amount of time working on the problem without writing a line of code or even looking at the data.\n",
    "\n",
    "**Thinking about and documenting the problem we're working on is an important step to performing effective data analysis that often goes overlooked.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Checking the data\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "The next step is to look at the data we're working with. Even curated data sets from the government can have errors in them, and it's vital that we spot these errors before investing too much time in our analysis.\n",
    "\n",
    "Generally, we're looking to answer the following questions:\n",
    "\n",
    "* Is there anything wrong with the data?\n",
    "* Are there any quirks with the data?\n",
    "* Do I need to fix or remove any of the data?\n",
    "\n",
    "Let's start by reading the data into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required packages \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Plotting Libraries \n",
    "%matplotlib nbagg\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator)\n",
    "\n",
    "# Data Analysis libraries\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset \n",
    "\n",
    "well_log = pd.read_csv('dataset/well_log_data.csv', header = 0 )\n",
    "well_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data seems to be in a usable format. There are 11 columns available to us.\n",
    "\n",
    "The first row in the data file defines the column headers, and the headers are descriptive enough for us to understand what each column represents. Some of the headers even give us the units that the measurements were recorded in, just in case we needed to know at a later point in the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**One of the first things we should do is to look at each of the measurement and see if there are any missing values, outliers or data type mismatches.** \n",
    "\n",
    "To conduct this quick analysis we will use the open source library **'pandas-profiling'** which can quickly do an exploratory data analysis with just a few lines of code.\n",
    "We will generate a pandas profile and output a report in the notebook for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas profile \n",
    "\n",
    "profile = ProfileReport(well_log, title=\"Pandas Profiling Report\")\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important findings from the overview <br>\n",
    "\n",
    "Data Statistics: <br>\n",
    "1. There are 11 variables/measurements and 331 observations/depth values \n",
    "2. There are 10 cells which contain missing values \n",
    "<br>\n",
    "\n",
    "Variable Types: <br>\n",
    "\n",
    "1. There are 10 Numerical and 1 Categorical variable\n",
    "\n",
    "Warnings: <br>\n",
    "1. Neutron Posority has all constant values of -999.25, this doesnt look okay as the formation cannot have constant neutron porosity value of -999.25. This column was being considered a Categorical variable because of just one constant value.\n",
    "2. Gamma Ray has 10 missing entries, we need to find and fill those missing entries. \n",
    "3. Density has 5 Zero entries which is suspicious as the density cannot be zero.\n",
    "\n",
    "Let us look at the other items to see if there are any other data issues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables \n",
    "\n",
    "There is a lot of information about each of the variable in the Variables section but we are trying to identify any data related issues which we need to correct before we move forward with the analysis.\n",
    "\n",
    "You can verify the findings we had in the Data overview section by looking at the distributions of each of the variables. Additional observations not covered in the Data Overview section are: <br>\n",
    "1. The depth seems to show some values close to 6000m as seen in the distribution whereas all the other depth values are close to 2000m. It is not possible for the depth to change from 2000 to 6000 m for short period of time in the 330 m well section. This most likely is bad entry or an outlier. <br>\n",
    "2. Neutron porosity which we noticed above contains a constant value of -999.25 has been rejected as a bad entry by the algorithm already.\n",
    "\n",
    "Feel free to look and interpret **Interactions** section to see scatter plots between different measurements. <br>\n",
    "We will concentrate on the **Correlations** section to look at dependencies between the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple correlation coefficients that you can check. For instance the widely known pearson correlation coefficient provides the linear correlation coefficient but in case you believe the variables are not linearly associated you may check spearman correlation coefficient which captures non linear monotonic correlations quite well.\n",
    "For now we are only looking at the pearson correlation coefficient. <br>\n",
    "1. Gamma Ray and Shale Volume show a strong positive correlation which could be the case as one of the inputs for the shale volume computation is Gamma Ray and  also shale formations in general have higher Gamma Ray counts. <br>\n",
    "2. There is a negative correlation between resistivity and density porosity and a positive correlation between sonic and density porosity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Tidying the data\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exploratory data analysis carried out above indicates the need to do some data cleaning before we proceed. Below are the steps for data cleaning for each of the variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute Missing Values \n",
    "\n",
    "Missing values in the column **Gamma Ray** column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_log[well_log['GammaRay(API)'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not ideal that we had to drop those rows, especially considering they're all sequential as far as the depth of the well is concerned. Since it seems like the missing data is systematic — all of the missing values are in the same column this error could potentially bias our analysis.\n",
    "\n",
    "Most frequently used approaches to handle the missing data are: <br>\n",
    "1. Do nothing\n",
    "2. Drop entire row \n",
    "3. Fill with the mean or median  value of the measurement for entire well section\n",
    "4. Fill with the previous or the next value to missing entry \n",
    "4. Interpolate between the neighbouring values(value just before and after the missing entry) \n",
    "5. Impute using k-NN \n",
    "\n",
    "Depending on the used case you can decide which approach works the best. In this case we will use the k-NN approach to fill the values. In the k-NN approach the other available meaurements (Shale Volume, Resistivity, Sonic) will be used to determine the likely values of Gamma Ray in the missing interval. <br>\n",
    "Ref: [Nearest Neighbors Imputation](https://scikit-learn.org/stable/modules/impute.html#nearest-neighbors-imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN to determine the missing Gamma Ray values \n",
    "\n",
    "nan = np.nan\n",
    "X = np.array(well_log.loc[:, ['GammaRay(API)','ShaleVolume','Resistivity','Sonic']])\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "well_log['GammaRay_Imputed'] = imputer.fit_transform(X)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the missing Gamma Ray values \n",
    "\n",
    "well_log.loc[297:312, ['GammaRay(API)','GammaRay_Imputed']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers or Incorrect values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Density** column has some zero entries and the **Depth** also seems to have some outlier values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Density "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Histogram to confirm zero density values \n",
    "\n",
    "well_log.Density.hist()\n",
    "plt.title('Distribution of Density Values')\n",
    "plt.xlabel('Density Values (gm/cc)')\n",
    "plt.ylabel('Count of the entries')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are density values around 0. We can locate them and replace with neighbouring values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_log[well_log.Density ==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are very few entries which are zero. We can just replace the 0 density value with the last non zero density. In the below code snippet, method = 'ffill' replaces the 0 values with the last non zero density value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_log.Density.replace(0, method = 'ffill', inplace = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Histogram to confirm zero density values \n",
    "well_log.Density.hist()\n",
    "plt.title('Distribution of Density Values')\n",
    "plt.xlabel('Density Values (gm/cc)')\n",
    "plt.ylabel('Count of the entries')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Density Values look reasonable now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Depth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Histogram to confirm outliers in depth values \n",
    "\n",
    "well_log['Depth(m)'].plot()\n",
    "plt.title('Depth interval(m)')\n",
    "plt.ylabel('Depth(m)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The depth value in the well section shows a sudden jump to 6000m+ (outlier) which does not seem to be right. It could be an error in recording data. We need to smoothen it out and remove these outlier values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Identify the outlier values \n",
    "# 2. Assign NaN values to these outliers\n",
    "# 3. Interpolate(linear) based on the surrounding depth values\n",
    "\n",
    "well_log.loc[ well_log['Depth(m)']>6000,'Depth(m)' ] = np.nan\n",
    "transformed_depth = well_log['Depth(m)'].interpolate(method = 'linear')\n",
    "well_log['Depth(m)_imputed'] = transformed_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_log.loc[300:310, ['Depth(m)','Depth(m)_imputed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputed Depth values \n",
    "\n",
    "well_log['Depth(m)_imputed'].plot()\n",
    "plt.title('Depth interval(m) after imputation')\n",
    "plt.ylabel('Depth(m)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the depth seems to be okay. The well section is between 2300 and 2350 m depth interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove redundant columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Neutron Porosity is constant value -999.25 through out, it seems to be redundant and can be removed from the analysis. Also going forward we can use the imputed Gamma Ray and Depth values hence we do not need th original columns. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_log.drop(columns = ['GammaRay(API)', 'NeutronPorosity','Depth(m)'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is in a clean form, we can proceed with the Rock Typing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Rock Type Analysis\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For rock typing we can use any unsupervised clustering technique. **k-means** is the most widely used clustering technique which samples the dataset into k groups/clusters of equal variance by minimizing the criteria called **Inertia** or **within-cluster-sum-of-squares**.  <br>\n",
    "Refer: [sklearn-k-means-Clustering](https://scikit-learn.org/stable/modules/clustering.html#k-means) <br>\n",
    "\n",
    "Prior to implementation of k-means, the data needs to be normalized as otherwise the clusters tend to separate along the variables with higher variance. <br>\n",
    "Refer: [feature_scaling_k-means](https://stats.stackexchange.com/questions/21222/are-mean-normalization-and-feature-scaling-needed-for-k-means-clustering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the dataset \n",
    "\n",
    "logs_tmp = well_log.values  \n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(logs_tmp)\n",
    "well_logs_scaled = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = []\n",
    "clusters_list = range(1,11)\n",
    "\n",
    "# Compute Inertia value for different number of cluster\n",
    "for num_clusters in clusters_list:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model=KMeans(n_clusters=num_clusters , random_state=123)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(well_logs_scaled)\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "\n",
    "# Plot the Inertia Vs Number of Clusters \n",
    "plt.plot(clusters_list, inertias, marker = 'o')\n",
    "\n",
    "plt.xlabel('number of clusters, k', size=20)\n",
    "plt.ylabel('inertia', size=20)\n",
    "plt.xticks(clusters_list)\n",
    "plt.title('Interia Vs Number of clusters')\n",
    "plt.tick_params(direction='out', length=6, width=2, colors='k')\n",
    "plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "plt.grid( zorder=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Elbow method** is used to choose the appropriate number of clusters. The target is to define the number of clusters such that the total intra cluster variation [or **Inertia:** total within-cluster sum of square (WSS)]  is minimized that shows that the points within a cluster are similar to each other.\n",
    "\n",
    "The Elbow method looks at the total WSS as a function of the number of clusters: One should choose a number of clusters so that adding another cluster doesn’t improve much better the total WSS. In our case the maximum drop in the Inertia happens up to 3 clusters. After 3 the inertia drop is not very significant. We have sufficient evidence to go with k = 3.\n",
    "\n",
    "Let us now implement the k-means with 3 clusters and predict the 3 Rock types for the well section. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means implementation with 3 clusters\n",
    "\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(well_logs_scaled)\n",
    "\n",
    "labels_rocks = kmeans.predict(well_logs_scaled)\n",
    "rocktypes = pd.DataFrame({'RockType':labels_rocks})\n",
    "well_log['rock_types'] = rocktypes.RockType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_logs_scaled_embedded = TSNE(n_components=2, learning_rate=200,random_state=10, \n",
    "                                 perplexity=50).fit_transform(well_logs_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projecting the well log features into 2d projection using t-SNE\n",
    "\n",
    "classes = ['RockType 1','RockType 2','RockType 3']\n",
    "scatter = plt.scatter(well_logs_scaled_embedded[:,0],\n",
    "            well_logs_scaled_embedded[:,1], \n",
    "            c =  rocktypes['RockType'], cmap = 'plasma' )\n",
    "plt.title('t-SNE 2D Projection of well logs')\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=classes)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3 rock types identified can be seen pretty well on the 2D projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA - Low dimensional projection of the rock types with loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA helps in visualizing the data in a low dimension (2D or 3D). The advantage of using PCA is that we can visualize how the data points are spread across the features in our dataset. However, keep in mind that PCA applies to only linearly separable data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of it as Google Maps. If you move in a certain direction of a feature, the data points (rocks) become stronger with respect to that feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA coupled with the cluster labels generated by the clustering variables helps us understand the data in a much better fashion. The distance between the points can be interpreted with respect to features as well. <br>\n",
    "Since our data is linearly separable, let us visualize our dataset using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above installation goes into the requirements file and the library imports goes into the import section of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pca import pca\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = well_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace the rock types with class descriptions\n",
    "X_pca = X_pca.replace({'rock_types' : { 0 : 'RockType 1', 1 : 'RockType 2', 2 : 'RockType 3' }})\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#Standardize all variables but retain the dataframe structure\n",
    "X_pca[['ShaleVolume', 'Resistivity', 'Sonic', 'Vp(m/s)', 'Vs(m/s)', 'Density',\n",
    "       'DensityPorosity(g/cc)', 'PoissonRatio', 'GammaRay_Imputed',\n",
    "       'Depth(m)_imputed']] = scaler.fit_transform(X_pca[['ShaleVolume', 'Resistivity', 'Sonic', 'Vp(m/s)', 'Vs(m/s)', 'Density',\n",
    "       'DensityPorosity(g/cc)', 'PoissonRatio', 'GammaRay_Imputed',\n",
    "       'Depth(m)_imputed']])\n",
    "\n",
    "#Set the index of dataframe as the labels we require to display the rock types\n",
    "X_pca = X_pca.set_index('rock_types')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize PCA - Let's see it in 2D\n",
    "model = pca(n_components=2)\n",
    "results = model.fit_transform(X_pca)\n",
    "fig, ax = model.biplot(n_feat=10, label = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the above chart, the data points (rock types) can be visualized along with the features.\n",
    "Do not focus on the x and y axes, as they are just components derived to explain the variation in data in 2 dimensions.<br>\n",
    "Here PC1 and PC2 together explain 86% of the variation in data (which in fact in a good number!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to read the direction of PCA arrows: <br>\n",
    "<b>1. Towards the direction of an arrow:</b> As you can see, Rock Type 1 data points are towards the direction of DensityPorosity. When you move in the direction of arrow head of a feature, it means that the values of that feature for the data point increases. This means that Rock Type 1 had high values of Density Porosity.<br><br>\n",
    "<b>2. Opposite to the direction of arrow:</b> When you move opposite to the direction of the arrowhead of a feature, it means that the data point has low unit values for that feature. For example, Rock Type 1 has low values for Resistivity.<br><br>\n",
    "<b>3. Feature arrowheads orthogonal to each other:</b> Orthogonal arrows mean that they are not related to each other. For example - Shale Volume is not related to Sonic<br><br>\n",
    "<b>4. 2 Feature arrows in the same direction:</b> This means they are highly correlated to each other. For example - Shale Volume and GammaRay. Consider removing them in case of classification problems (if we take the labels created by this unsupervised learning to predict data points or rock types in the future)<br><br>\n",
    "<b>5. 2 Feature arrows in opposite direction:</b> This means they are negatively correlated. For example - Resistivity is negatively related to DensityPorosity<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coupled with clustering, PCA also helps in determining the features that help determine a rock type. For example, Rock Type 1 is characterized by high DensityPorosity, while Rock Type 3 is characterized by high values in Sonic, PoissonRatio and ShaleVolume.\n",
    "Rock Type 2 is characterized by high values in Vs, Vp, Resistivity and Density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot well logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the well logs \n",
    "\n",
    "def well_log_display(top_depth,bottom_depth, df, list_columns):\n",
    "    \n",
    "    logs=df[(df['Depth(m)_imputed'] >= top_depth) & (df['Depth(m)_imputed'] <= bottom_depth)]\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=len(list_columns), figsize=(15,20),  sharey=True)\n",
    "    fig.subplots_adjust(top=0.75,wspace=0.1)\n",
    "\n",
    "    for axes in ax:\n",
    "        axes.set_ylim (top_depth,bottom_depth)\n",
    "        axes.yaxis.grid(True)\n",
    "        axes.get_xaxis().set_visible(False) \n",
    "        \n",
    "    for i in range(len(list_columns)):\n",
    "        if (list_columns[i] == 'rock_types'):\n",
    "            ax1=ax[i].twiny()\n",
    "            label = logs.rock_types\n",
    "            tagged_depths = logs['Depth(m)_imputed']\n",
    "            rock_type_depth(ax1,tagged_depths, label)\n",
    "            ax1.set_xlabel(list_columns[i],color= list(colors.keys())[i], fontsize=15)    \n",
    "            ax1.minorticks_off()\n",
    "            ax1.grid(False)\n",
    "            \n",
    "        elif (list_columns[i] == 'Resistivity'):\n",
    "            ax1=ax[i].twiny()\n",
    "            ax1.set_xlim(0.1,10)\n",
    "            ax1.set_xscale('log')\n",
    "            ax1.grid(True)\n",
    "            ax1.plot(logs[list_columns[i]], logs['Depth(m)_imputed'], label=list_columns[i], color=list(colors.keys())[i]) \n",
    "            ax1.spines['top'].set_position(('outward',40))\n",
    "            ax1.set_xlabel(list_columns[i],color= list(colors.keys())[i], fontsize=10)    \n",
    "            ax1.tick_params(axis='x', colors=list(colors.keys())[i],labelsize = 10)\n",
    "        \n",
    "        else:\n",
    "            ax1=ax[i].twiny()\n",
    "            ax1.grid(True)\n",
    "            ax1.plot(logs[list_columns[i]], logs['Depth(m)_imputed'], label=list_columns[i], color=list(colors.keys())[i]) \n",
    "            ax1.spines['top'].set_position(('outward',40))\n",
    "            ax1.set_xlabel(list_columns[i],color= list(colors.keys())[i], fontsize=10)    \n",
    "            ax1.tick_params(axis='x', colors=list(colors.keys())[i],labelsize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the rock type track \n",
    "\n",
    "def rock_type_depth (ax, depth, label):\n",
    "    patches = []\n",
    "    for i in range(len(well_log)):\n",
    "        poly=Rectangle((0,depth[i]), 4, 1)\n",
    "        patches.append(poly)\n",
    "\n",
    "    p = PatchCollection(patches, match_original=False,alpha=0.6)\n",
    "\n",
    "    colors = 1000*np.array(well_log.rock_types)\n",
    "    p.set_array(np.array(colors))\n",
    "    ax.add_collection(p)\n",
    "    ax.invert_yaxis()\n",
    "    x_pos_label=['', '','','']\n",
    "    ax.set_xticklabels(x_pos_label, rotation = 0)\n",
    "    ax.get_xaxis().set_visible(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the combined plot for 'ShaleVolume', 'Resistivity', 'GammaRay_Imputed','Density', 'rock_types'\n",
    "\n",
    "list_columns = ['ShaleVolume', 'Resistivity', 'GammaRay_Imputed','Density', 'rock_types']\n",
    "well_log_display(well_log['Depth(m)_imputed'].min(),well_log['Depth(m)_imputed'].max(), well_log, list_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each colour corresponds to a particular rock type/lithology identified by the unsupervised machine learning algorithm. You can further intepret the kind of rocks corresponding to each of the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have completed the Machine learning pipeline to demo rock type analysis starting from Exploratory Data Analysis to Rock type identification using unsupervised machine learning.\n",
    "\n",
    "If you liked this notebook, you'll love the way Digital Hub can transform you projects and empower your data science team! Discover a cutting edge new platform that puts the absolute best tools in your hands.\n",
    "\n",
    "We hope you will feel more confident now to replicate your understanding to other similar used cases. <br>\n",
    "For any questions please get intouch with the **Digital Hub team** at **support@digitalhub.io**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **pandas-profiling** - Generates profile reports from a pandas DataFrame<br> [LICENSE](https://github.com/pandas-profiling/pandas-profiling/blob/master/LICENSE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
